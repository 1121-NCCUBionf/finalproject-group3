{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Kbb-MleQ_Qhj",
        "kHIm5ZBr-BWL",
        "cYO6rG8z_xtK"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to Bioinformatics final project!\n",
        "\n",
        "This **\"tutorial colab notebook\"** are created and organized by group 3: Christofer Julio and Zihao Chen.\n",
        "Our task for the bioinformatics final project is to mimic the result of this paper:\n",
        "\n",
        "```\n",
        "Meng-Shin Shiao, Jia-Ming Chang, Wen-Lang Fan, Mei-Yeh Jade Lu, Cedric Notredame, Shu Fang, Rumi Kondo, Wen-Hsiung Li, Expression Divergence of Chemosensory Genes between Drosophila sechellia and Its Sibling Species and Its Implications for Host Shift , Genome Biology and Evolution, Volume 7, Issue 10, October 2015, Pages 2843â€“2858, https://doi.org/10.1093/gbe/evv183\n",
        "```\n",
        "However, due to the limitation of the resources, we are only able to replicate the **1 sample set each**.\n",
        "\n",
        "\n",
        "**Advantages** in using colab notebook:\n",
        "\n",
        "\n",
        "1.   First, colab notebook has its convenience way to install packages from various systems. In our case, we use bioconda and original python packages to process the data, and it works pretty well. However, remember to install everything first before we start.\n",
        "2.   Second, colab works wonderful for some people who has low computing resource (old notebook/PC), as colab actually has 12 GB RAM, T4 GPU, and 100 GB disk space\n",
        "3. Third, since we are not using any GPU resource, we can actually connect to colab for way longer time compared to people who intend to utilize colab for Deep learning.\n",
        "4. You can run several codes simultaneously.\n",
        "\n",
        "Now, here are some **tips**:\n",
        "\n",
        "\n",
        "1.   Make sure to delete the runtime often, since processing big alignment file will take a lot of the disk space, and it can result in automatic stopping from the system once all the disk space are gone.\n",
        "2.   Make sure you have a big google drive spaces, especially when you are dealing with big data. In our case, we need more than 1 TB of disk space if we use all the dataset (Google drive has 2TB subscription).\n",
        "3. One of the main disadvantages of Google colab is that you need to upload the files here. And it will take a very long time for each file sometimes, especially when you don't have good internet connection.\n",
        "\n",
        "\n",
        "**before we start, run this to download and configure conda environment.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Kbb-MleQ_Qhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.8.2-Linux-x86_64.sh && \\\n",
        "chmod +x Miniconda3-py37_4.8.2-Linux-x86_64.sh && \\\n",
        "bash ./Miniconda3-py37_4.8.2-Linux-x86_64.sh -b -f -p /usr/local\n",
        "\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')\n"
      ],
      "metadata": {
        "id": "xHy1Mtoq_OUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# install required packages and import the data from google drive\n",
        "\n",
        "\n",
        "Our package mainly comes from bioconda (for fastq files processing, hisat2, cufflink, etc.), and some of them are imported from python packages (especially when doing plotting, creating dataframe, etc).\n",
        "\n",
        "In addition, we need to mount google drive to google colab, since our fastq files are saved there."
      ],
      "metadata": {
        "id": "NMM5oU1V8wAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fastqc is used for checking the quality of the sequence\n",
        "!conda install -c bioconda fastqc -y\n",
        "\n",
        "# trimomatic is used for trim the sequence\n",
        "! conda install -c bioconda trimmomatic -y\n",
        "\n",
        "# hisat2 for aligning sequence\n",
        "# if it doesn't work, install conda hisat2 first, and uninstall conda hisat2. It suddenly works somehow.\n",
        "!apt-get install hisat2\n",
        "\n",
        "# install subreads for removing duplicates and feature counts\n",
        "!conda create -n featurecounts -c bioconda subread -y"
      ],
      "metadata": {
        "id": "1nlAYRWjWtvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run this to install samsul\n",
        "\n",
        "!apt-get install libssl1.0.0\n",
        "\n",
        "# Download and install samtools\n",
        "!wget https://github.com/samtools/samtools/releases/download/1.10/samtools-1.10.tar.bz2\n",
        "!tar xjf samtools-1.10.tar.bz2\n",
        "%cd samtools-1.10\n",
        "!make\n",
        "!make install"
      ],
      "metadata": {
        "id": "A1iiY_a99wkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install cufflinks\n",
        "!apt-get install -y libhdf5-dev\n",
        "\n",
        "# Download and install Cufflinks\n",
        "!wget http://cole-trapnell-lab.github.io/cufflinks/assets/downloads/cufflinks-2.2.1.Linux_x86_64.tar.gz\n",
        "!tar -zxvf cufflinks-2.2.1.Linux_x86_64.tar.gz\n",
        "!mv cufflinks-2.2.1.Linux_x86_64/* /usr/local/bin/\n"
      ],
      "metadata": {
        "id": "FxDgcvYDNHxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import required python packages\n",
        "from google.colab import drive\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "import pandas as pd\n",
        "import numpy"
      ],
      "metadata": {
        "id": "iTUd3KlLBSmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mounting google colab drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKriE0RHSm-h",
        "outputId": "e369b697-0489-44b2-9714-3397714b43b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check your data with fastqc\n",
        "\n",
        "Fastqc is used for checking the quality of the sequence. Bad sequence quality will affect the accuracy of the mapping, and the quality of the alignment in general. This is why checking the quality with Fastqc is an important step before we proceed to the next step."
      ],
      "metadata": {
        "id": "kHIm5ZBr-BWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# put your file path in the file_path variable\n",
        "\n",
        "def fastqc(file_path):\n",
        "  !fastqc {file_path}\n",
        "\n",
        "# run this code if you intend to check fastqc on all files:\n",
        "\n",
        "def fastqc(file_path):\n",
        "  for fasta in os.listdir(file_path):\n",
        "    fasta_w_path = os.path.join(file_path, fasta)\n",
        "    !fastqc {fasta_w_path}\n"
      ],
      "metadata": {
        "id": "mDmxBttm-FUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Trimmomatic to trim the \"bad\" sequences\n",
        "\n",
        "As we have mentioned before, low-quality sequence can be bad for your overall research project. Hence, what should we do to improve our sequence quality? One of the most common way to make our sequence better is to trim the sequence.\n",
        "\n",
        "Trimming low-quality sequence can be beneficial, especially to improve the quality of the data. It is also helped to decrease the probability of sequencing errors, PCR bias, or other factors such as poor sample quality.\n",
        "\n",
        "In our case, after some trial and error, we decide to use this settings:\n",
        "\n",
        "```\n",
        "param = {'SLIDINGWINDOW':\"4:20\", \"MINLEN\": 60}\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "However, if the sequence is \"good enough\" according to Fastqc, we can skip this step."
      ],
      "metadata": {
        "id": "cYO6rG8z_xtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trim sequences for all the _2 fasta files\n",
        "\n",
        "def trim(path_source, path_dest, fastq_files_names, param):\n",
        "\n",
        "  for fastq in fastq_files_names:\n",
        "\n",
        "    # create path source and path destination\n",
        "    path_source_names = os.path.join(path_source, fastq)\n",
        "    path_dest_names = os.path.join(path_dest, fastq)\n",
        "\n",
        "    # trimmomatic command\n",
        "    !trimmomatic SE -threads 8 -phred33 {path_source_names} {path_dest_names} SLIDINGWINDOW:{param[\"SLIDINGWINDOW\"]} MINLEN:{param[\"MINLEN\"]}\n"
      ],
      "metadata": {
        "id": "pr3SCLQj_FRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# declare path\n",
        "path_source = r'/content/drive/MyDrive/fastaq/D_sim_jp'\n",
        "path_dest = r'/content/drive/MyDrive/fastaq/D_sim_jp_trimmed'\n",
        "\n",
        "# the second fastq files have bad sequences in general so we list it here\n",
        "fastq_files_names = [\"SRR1973492_2.fastq.gz\", \"SRR1973495_2.fastq.gz\"]\n",
        "\n",
        "# parameter for the trimming we use\n",
        "param = {'SLIDINGWINDOW':\"4:20\", \"MINLEN\": 60}\n",
        "\n",
        "trim(path_source, path_dest, fastq_files_names, param)"
      ],
      "metadata": {
        "id": "5w0oQjUZFdNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Index, alignment with HISAT2\n",
        "\n",
        "Then, once we have bunch of nice sequences, what we need to do next is to align the sequences with the reference genomes. Reference genomes, as its name implies, is used as the standardized genome for comparing it with the sampled genomes.\n",
        "\n",
        "Hence, by aligning the sampled genomes with the standardized genomes, researchers will have a better understanding of the genetic variations, relationships, and similarities between the sampled genomes and the reference genomes.\n",
        "\n"
      ],
      "metadata": {
        "id": "T8gCuAiXgkdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # hisat2 build index\n",
        "\n",
        "def hisat2_build(path_source, path_dest, files=list()):\n",
        "\n",
        "  # check if the path exist\n",
        "  os.makedirs(path_dest, exist_ok=True)\n",
        "\n",
        "  path_source_names = os.path.join(path_source, files[0])\n",
        "  path_dest_names = os.path.join(path_dest, files[1])\n",
        "  !hisat2-build -p 8 {path_source_names} {path_dest_names}\n",
        "\n",
        "  print('index building finished!')"
      ],
      "metadata": {
        "id": "I5gv3mbJG6oF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hisat2_align(genome_index_folder, input_directory, output_directory):\n",
        "\n",
        "    #  check if the path exist\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "    all_files = os.listdir(input_directory)\n",
        "\n",
        "    # Filter for forward and reverse reads separately\n",
        "    forward_reads = [file for file in all_files if file.endswith('_1.fastq.gz')]\n",
        "    reverse_reads = [file for file in all_files if file.endswith('_2.fastq.gz')]\n",
        "    print(reverse_reads)\n",
        "\n",
        "    # Loop through each forward read\n",
        "    for forward_read in forward_reads:\n",
        "        forward_reads_path = os.path.join(input_directory, forward_read)\n",
        "        filename, type1, type2 = forward_read.split('.')\n",
        "        output_sam = os.path.join(output_directory, f\"{filename}_alignment.sam\")\n",
        "\n",
        "        # HISAT2 command for forward read\n",
        "        hisat2_command = f\"hisat2 -x {genome_index_folder}/genome_index -U '{forward_reads_path}' -S '{output_sam}'\"\n",
        "        os.system(hisat2_command)\n",
        "\n",
        "        print(f\"HISAT2 alignment completed for {forward_read}\")\n",
        "\n",
        "    # Loop through each reverse read\n",
        "    for reverse_read in reverse_reads:\n",
        "        reverse_reads_path = os.path.join(input_directory, reverse_read)\n",
        "        filename, type1, type2 = reverse_read.split('.')\n",
        "        output_sam = os.path.join(output_directory, f\"{filename}_alignment.sam\")\n",
        "\n",
        "        # HISAT2 command for reverse read\n",
        "        hisat2_command = f\"hisat2 -x {genome_index_folder}/genome_index -U '{reverse_reads_path}' -S '{output_sam}'\"\n",
        "        os.system(hisat2_command)\n",
        "\n",
        "        print(f\"HISAT2 alignment completed for {reverse_read}\")"
      ],
      "metadata": {
        "id": "Da9997SZFfm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_source = r'/content/drive/MyDrive/fastaq/reference_genome'\n",
        "path_dest = r'/content/drive/MyDrive/fastaq/reference_genome_index'\n",
        "files=['dsim-all-chromosome-r1.3.fasta', 'genome_index']\n",
        "\n",
        "hisat2_build(path_source, path_dest, files)"
      ],
      "metadata": {
        "id": "RTyvWFI3x2Dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genome_index_folder = r'/content/drive/MyDrive/fastaq/d_melan_alg_process/genome_index_melan'\n",
        "input_directory = r'/content/drive/MyDrive/bio_dataset/dataset/D_melanogaster_Taiwan'\n",
        "output_directory = r'/content/drive/MyDrive/fastaq/D_melan_aligned'\n",
        "\n",
        "hisat2_align(genome_index_folder, input_directory, output_directory)"
      ],
      "metadata": {
        "id": "8_UKtADp795h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e774a610-f669-4524-8407-e740a7a50262"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['SRR5638358_2.fastq.gz']\n",
            "HISAT2 alignment completed for SRR5638358_2.fastq.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FPKM Gene Expression with samtools and cufflinks\n",
        "\n",
        "In order to generate the FPKM gene expression quantifications of the alignment result, we utilize cufflinks. However, since we are using **SAM** files, it needs to be converted first to **BAM** files, and samtools provide some convenient command to do so. Aside from the alignment files, we also need an additional annotated gene data, which is usually saved with gtf. format. The implementation of the package can be seen on the code below.\n",
        "\n",
        "Additionally, we can directly generates FPKM based on the raw count we have.Using subreads, we generate the SAM files and convert it to the raw counts. After we are able to generate raw counts, we can calculate the FPKM based on the formula below:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "FPKM = (Number of Fragments Mapped to Gene) / (Gene Length in Kilobases * Total Number of Fragments Mapped to All Genes in Millions)'\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A02DecixuWYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def counting_raw_count(gtf_file, output_dir, sam_dir, file_pairs, program_path):\n",
        "\n",
        "  os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "  # Define SAM file paths for forward and reverse reads\n",
        "  forward_sam_file = os.path.join(sam_dir, file_pairs[0])\n",
        "  reverse_sam_file = os.path.join(sam_dir, file_pairs[1])\n",
        "\n",
        "  name = file_pairs[0].split('_')[0]\n",
        "\n",
        "  # Output file name\n",
        "  output_combined_counts = name + '.txt'\n",
        "\n",
        "  # Run featureCounts for both forward and reverse reads\n",
        "  featurecounts_command = (\n",
        "      f\"{program_path} -a {gtf_file} \"\n",
        "      f\"-o {output_combined_counts} \"\n",
        "      f\"-F GTF -p -B -C -f -T 4 -g ID \"\n",
        "      f\"{forward_sam_file} {reverse_sam_file}\"\n",
        "  )\n",
        "  os.system(featurecounts_command)\n",
        "\n",
        "  print(\"FeatureCounts completed for both forward and reverse reads.\")\n",
        "\n",
        "def remove_duplicates(input_sam, output_sam, filename):\n",
        "  # split name with type\n",
        "  name = filename.split('.')[0]\n",
        "\n",
        "  # create input and output\n",
        "  input_sam_name = os.path.join(input_sam, filename)\n",
        "  output_sam_name = os.path.join(output_sam, f\"{name}_noduplicates.sam\")\n",
        "\n",
        "\n",
        "  # remove dupes\n",
        "  !samtools rmdup {input_sam_name} {output_sam_name}\n",
        "  print(\"duplicates removed.\")\n",
        "\n",
        "\n",
        "def FPKM_cufflink(forward_path, reverse_path, save_path):\n",
        "\n",
        "  !samtools view -bS -o forward.bam {forward_path}\n",
        "  !samtools view -bS -o reverse.bam {reverse_path}\n",
        "\n",
        "  # Sort and index BAM files\n",
        "  print('processing forward bam file')\n",
        "  !samtools sort -o forward_sorted.bam forward.bam\n",
        "  !samtools index forward_sorted.bam\n",
        "\n",
        "  print(\"processing reverse bam file\")\n",
        "  !samtools sort -o reverse_sorted.bam reverse.bam\n",
        "  !samtools index reverse_sorted.bam\n",
        "\n",
        "  # Run Cufflinks separately on each strand\n",
        "  print(\"run cufflink\")\n",
        "  !cufflinks -p 4 -o forward_output --library-type fr-firststrand forward_sorted.bam\n",
        "  !cufflinks -p 4 -o reverse_output --library-type fr-secondstrand reverse_sorted.bam\n",
        "\n",
        "  # Merge assemblies\n",
        "  print(\"merge output\")\n",
        "  !cuffmerge -o merged_output assemblies.txt\n",
        "\n",
        "  # Run Cuffquant on merged assemblies and original BAM files\n",
        "  print('merge assemblies')\n",
        "  !cuffquant -p 4 -o quantification_output merged_output/merged.gtf forward_sorted.bam\n",
        "  !cuffquant -p 4 -o quantification_output merged_output/merged.gtf reverse_sorted.bam\n",
        "\n",
        "  # Run Cuffnorm to normalize expression values\n",
        "  print(\"normalize expression values\")\n",
        "  !cuffnorm -p 4 -o cuffnorm_output merged_output/merged.gtf quantification_output/forward_sorted/abundances.cxb quantification_output/reverse_sorted/abundances.cxb\n",
        "\n",
        "  print('saving data')\n",
        "  !cp cuffnorm_output/genes.fpkm_table.csv {save_path}\n",
        "\n",
        "  print('data saved!')\n",
        "\n",
        "\n",
        "def raw_counts_to_fpkm(raw_counts_file, output_csv_file):\n",
        "    # Read raw counts from the FeatureCounts output file\n",
        "    raw_counts_df = pd.read_csv(raw_counts_file, sep=' ', index_col=0, skiprows=1)\n",
        "\n",
        "    # Calculate total mapped reads\n",
        "    total_mapped_reads = raw_counts_df.sum(axis=0).sum()\n",
        "\n",
        "    # Calculate feature lengths from raw counts\n",
        "    feature_lengths = raw_counts_df.sum(axis=0).astype(str)\n",
        "\n",
        "    # Calculate FPKM values\n",
        "    fpkm_df = (raw_counts_df / feature_lengths.astype(float)) / (total_mapped_reads / 1e6)\n",
        "\n",
        "    # Save FPKM values to a CSV file\n",
        "    fpkm_df.to_csv(output_csv_file)\n"
      ],
      "metadata": {
        "id": "7IQYVVdrcIsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove dupes\n",
        "file_name = r\"SRR1973492_2_alignment.sam\"\n",
        "input_sam = '/content/drive/MyDrive/fastaq/D_sim_jp_aligned'\n",
        "output_sam =  '/content/drive/MyDrive/fastaq/removed_dup_d_sim_jp'\n",
        "\n",
        "remove_duplicates(input_sam, output_sam, file_name)"
      ],
      "metadata": {
        "id": "TtCN1b-JreIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gtf_files = r'/content/drive/MyDrive/fastaq/reference_genome/dsim-all-r1.3.gff.gz'\n",
        "output_dir = r'/content/drive/MyDrive/fastaq/reads'\n",
        "file_pairs = [\"SRR1973492_1_alignment.sam\", \"SRR1973492_2_alignment.sam\"]\n",
        "sam_dir = r'/content/drive/MyDrive/fastaq/D_sim_jp_aligned'\n",
        "\n",
        "# need to declare the path first\n",
        "subread_path = r'/usr/local/envs/featurecounts/bin/featureCounts'\n",
        "\n",
        "counting_raw_count(gtf_files, output_dir, sam_dir,file_pairs, subread_path)"
      ],
      "metadata": {
        "id": "4HYAxFyI4jRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_counts_file_path = '/content/drive/MyDrive/academic/bioinf/dataset/SRR1973492.txt'  # Replace with your actual raw counts file path\n",
        "output_csv_file_path = '/content/drive/MyDrive/academic/bioinf/dataset/simjp_fpkm_values.csv'  # Replace with your desired output CSV file path\n",
        "\n",
        "raw_counts_to_fpkm(raw_counts_file_path, output_csv_file_path)"
      ],
      "metadata": {
        "id": "O98k-LJItA6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalization across samples\n",
        "\n",
        "Since we have cross-samples (D. sech JP and D. sech TW), normalization technique needs to be used. In this case, we follow the settings explained in the paper, which is to utilize **q= 0.95** for upper quartile normalization. Before normalization, we change the gene names based on the OGS.\n",
        "\n",
        "In our case, we change the NOISeq package to Numpy for convenience, since numpy is a python package."
      ],
      "metadata": {
        "id": "Y1RN49_YjmUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the orthologous gene list from the document\n",
        "ogs_gene = pd.read_csv('/content/drive/MyDrive/academic/bioinf/dataset/OGS_GENES_3_SPECIES.csv')\n",
        "\n",
        "# Load FPKM Data for each species\n",
        "\n",
        "\n",
        "# species name order should be melan, sech, sech, sim\n",
        "def switch_gene_names(path_source, path_dest, ogs_gene, species_name=list()):\n",
        "\n",
        "  species_name = sorted(species_name)\n",
        "  fpkm_species1 = pd.read_csv(os.path.join(path_source, species_name[0]),delimiter='\\t', skiprows=0)\n",
        "  fpkm_species2 = pd.read_csv(os.path.join(path_source, species_name[1]),delimiter='\\t', skiprows=0)\n",
        "  fpkm_species3 = pd.read_csv(os.path.join(path_source, species_name[3]),delimiter='\\t', skiprows=0)\n",
        "  fpkm_species4 = pd.read_csv(os.path.join(path_source, species_name[4]),delimiter='\\t', skiprows=0)\n",
        "\n",
        "  ogs_gene.rename(columns={'#gene': 'Geneid'}, inplace=True)\n",
        "  ogs_melan = ogs_gene[['Geneid', 'Dmel']].copy()\n",
        "  ogs_sech = ogs_gene[['Geneid', 'Dsec']].copy()\n",
        "  ogs_sim = ogs_gene[['Geneid', 'Dsim']].copy()\n",
        "\n",
        "  melan_dict = ogs_melan.set_index('Dmel')['Geneid'].to_dict()\n",
        "  sech_dict = ogs_sech.set_index('Dsec')['Geneid'].to_dict()\n",
        "  sim_dict = ogs_sim.set_index('Dsim')['Geneid'].to_dict()\n",
        "\n",
        "  fpkm_species1['Geneid'] = fpkm_species1['Geneid'].map(melan_dict).fillna(fpkm_species1['Geneid'])\n",
        "  fpkm_species2['Geneid'] = fpkm_species2['Geneid'].map(sech_dict).fillna(fpkm_species2['Geneid'])\n",
        "  fpkm_species3['Geneid'] = fpkm_species3['Geneid'].map(sech_dict).fillna(fpkm_species3['Geneid'])\n",
        "  fpkm_species4['Geneid'] = fpkm_species4['Geneid'].map(sim_dict).fillna(fpkm_species4['Geneid'])\n",
        "\n",
        "  # Save the merged data as OGS CSV files\n",
        "  fpkm_species1.to_csv(os.path.join(path_dest, 'ogs_' + species_name[0]), index=False)\n",
        "  fpkm_species2.to_csv(os.path.join(path_dest, 'ogs_' + species_name[1]), index=False)\n",
        "  fpkm_species3.to_csv(os.path.join(path_dest, 'ogs_' + species_name[2]), index=False)\n",
        "  fpkm_species4.to_csv(os.path.join(path_dest, 'ogs_' + species_name[3]), index=False)"
      ],
      "metadata": {
        "id": "HIL0EnZgtHrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def upper_quartile_norm(path_source, path_dest, filename, q=95, combine=True):\n",
        "\n",
        "  # Load the gene expression counts table\n",
        "  if combine == True:\n",
        "    D_sech_tw = pd.read_csv(os.path.join(path_source, filename[0]), index_col=0)\n",
        "    D_sech_jp = pd.read_csv(os.path.join(path_source, filename[1]), index_col=0)\n",
        "    df = pd.concat([D_sech_tw, D_sech_jp], axis=0)\n",
        "\n",
        "  else:\n",
        "    df = pd.read_csv(os.path.join(path_source, filename), index_col=0)\n",
        "\n",
        "  # Define the columns for normalization\n",
        "  df['expression_column'] = df['FPKM']\n",
        "  fpkm = np.array(df['expression_column'])\n",
        "\n",
        "  # Calculate the upper quartile for each gene\n",
        "  upper_quartile = np.percentile(fpkm, 95)\n",
        "\n",
        "  # # # Normalize the gene expression data\n",
        "  # df[\"expression_columns\"] = df[\"expression_columns\"].div(upper_quartile, axis=1)\n",
        "\n",
        "  return df\n",
        "\n",
        "  # # Save the normalized counts table\n",
        "  # df.to_csv(os.path.join(path_dest,'normalized_counts_table_{name}.csv'))\n"
      ],
      "metadata": {
        "id": "UAYnLihfxsy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_source= r'/content/drive/MyDrive/fastaq/ogs'\n",
        "path_dest = r'/content/drive/MyDrive/fastaq/ogs_norm'\n",
        "filename = ['ogs_sechtw.csv', 'ogs_sechjp.csv']\n",
        "\n",
        "df = upper_quartile_norm(path_source, path_dest, filename, q=95, combine=True)\n",
        "df"
      ],
      "metadata": {
        "id": "_lDaKDmx9Lwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# creating the table and figures of upregulated and downregulated genes\n",
        "\n",
        "incoming"
      ],
      "metadata": {
        "id": "Ns9Ih6-Szz5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_upregulated_genes = ['Obp19a', 'Obp50a', 'Obp56d', 'CheA75a', 'CheA87a', 'Or23a',\n",
        "                             'Or35a', 'Or56a', 'Or67b', 'Or85b', 'Or85c', 'Gr64f', 'Ir84a']\n",
        "\n",
        "list_of_downregulated_genes = ['Obp83a', 'Obp99c', 'Obp99d', 'Or9a', 'Or42b']\n"
      ],
      "metadata": {
        "id": "drSAE-DcqMWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Expression Divergance\n",
        "\n",
        "We generated heat maps for the Or , Ir , Obp , and Gr gene families and also the whole antennal transcriptome for each sex in each species ( figs. 2â€“4 and supplementary figs. S2â€“S3 , Supplementary Material online). We calculated the average expression level (without upper quartile normalization) of each gene estimated from three sequencing lanes followed by ranking within each of the above four gene families. Duplicate genes excluded in the OGS were now included for expression profile analysis. Genes with lowest expression levels (FPKM = 0 in all the samples) were ranked as 1 (colored yellow in the heat maps), whereas those with highest expression levels were colored red in the heat maps. Hierarchical agglomerative clustering was applied to cluster species as well as genes for each gene family. The significance of clustering was tested by multiscale bootstrap resampling implemented in the pvclust R package ( Suzuki and Shimodaira 2006 )."
      ],
      "metadata": {
        "id": "WUI_jyHfz-oh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "8rnq7J7b4tuk"
      }
    }
  ]
}